{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3_K0GGSTrhd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This file has been modified by ROI Training, Inc. on 05/10/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VQkf8sFTeDo"
   },
   "source": [
    "# Getting Started with Text Embeddings + Vector Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overview and context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "\n",
    "In this tutorial, you learn how to use Google Cloud AI tools to quickly bring the power of Large Language Models to enterprise systems.  \n",
    "\n",
    "This tutorial covers the following -\n",
    "\n",
    "*   What are embeddings - what business challenges do they help solve ?\n",
    "*   Understanding Text with Vertex AI Text Embeddings\n",
    "*   Find Embeddings fast with Vertex AI Vector Search\n",
    "*   Grounding LLM outputs with Vector Search\n",
    "\n",
    "This tutorial is based on [the blog post](https://cloud.google.com/blog/products/ai-machine-learning/how-to-use-grounding-for-your-llms-with-text-embeddings), combined with sample code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Fu2OoUDTQ6w"
   },
   "source": [
    "### 2. Bringing Gen AI and LLMs to production services\n",
    "\n",
    "Many people are now starting to think about how to bring Gen AI and LLMs to production services, and facing with several challenges.\n",
    "\n",
    "- \"How to integrate LLMs or AI chatbots with existing IT systems, databases and business data?\"\n",
    "- \"We have thousands of products. How can I let LLM memorize them all precisely?\"\n",
    "- \"How to handle the hallucination issues in AI chatbots to build a reliable service?\"\n",
    "\n",
    "Here is a quick solution: **grounding** with **embeddings** and **vector search**.\n",
    "\n",
    "What is grounding? What are embedding and vector search? In this tutorial, we will learn these crucial concepts to build reliable Gen AI services for enterprise use. But before we dive deeper, let's try the demo below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1MAIOkCw35V"
   },
   "source": [
    "### 3. What are Embeddings?\n",
    "\n",
    "With the rise of LLMs, why is it becoming important for IT engineers and ITDMs to understand how they work?\n",
    "\n",
    "In traditional IT systems, most data is organized as structured or tabular data, using simple keywords, labels, and categories in databases and search engines.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img style=\"border: 1px solid gray\" \n",
    "         src=\"https://storage.googleapis.com/github-repo/img/embeddings/textemb-vs-notebook/1.png\" \n",
    "         width=\"650\">\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "In contrast, AI-powered services arrange data into a simple data structure known as \"embeddings.\"\n",
    "<div style=\"text-align: center;\">\n",
    "    <img \n",
    "         style=\"border: 1px solid gray\" \n",
    "         src=\"https://storage.googleapis.com/github-repo/img/embeddings/textemb-vs-notebook/2.png\"\n",
    "         width=\"650\"\n",
    "    >\n",
    "</div>\n",
    "\n",
    "Once trained with specific content like text, images, or any content, AI creates a space called \"embedding space\", which is essentially a map of the content's meaning.\n",
    "\n",
    "\n",
    "<div style=\"text-align: center; margin-bottom: 10px;\">\n",
    "    <img style=\"border: 1px solid gray\" \n",
    "         src=\"https://storage.googleapis.com/github-repo/img/embeddings/textemb-vs-notebook/3.png\"\n",
    "         width=\"650\">\n",
    "</div>\n",
    "\n",
    "AI can identify the location of each content on the map, that's what embedding is.\n",
    "                   \n",
    "<div style=\"text-align: center; margin-bottom: 10px;\">\n",
    "    <img style=\"border: 1px solid gray\" \n",
    "         src=\"https://storage.googleapis.com/github-repo/img/embeddings/textemb-vs-notebook/4.png\"\n",
    "         width=\"650\">\n",
    "</div>\n",
    "\n",
    "Let's take an example where a text discusses movies, music, and actors, with a distribution of 10%, 2%, and 30%, respectively. In this case, the AI can create an embedding with three values: 0.1, 0.02, and 0.3, in 3 dimensional space.\n",
    "\n",
    "<div style=\"text-align: center; margin-bottom: 10px;\">\n",
    "    <img style=\"border: 1px solid gray\" \n",
    "         src=\"https://storage.googleapis.com/github-repo/img/embeddings/textemb-vs-notebook/5.png\"\n",
    "         width=\"650\">\n",
    "</div>\n",
    "\n",
    "AI can put content with similar meanings closely together in the space.\n",
    "                   \n",
    "This is how Google organizes data across various services like Google Search, YouTube, Play, and many others, to provide search results and recommendations with relevant content.\n",
    "\n",
    "Embeddings can also be used to represent different types of things in businesses, such as products, users, user activities, conversations, music & videos, signals from IoT sensors, and so on.\n",
    "\n",
    "This is how Google organizes data across various services like Google Search, YouTube, Play, and many others, to provide search results and recommendations with relevant content.\n",
    "\n",
    "Embeddings can also be used to represent different types of things in businesses, such as products, users, user activities, conversations, music & videos, signals from IoT sensors, and so on.\n",
    "\n",
    "AI and Embeddings are now playing a crucial role in creating a new way of human-computer interaction.\n",
    "\n",
    "<div style=\"text-align: center; margin-bottom: 10px;\">\n",
    "    <img style=\"border: 1px solid gray\" \n",
    "         src=\"https://storage.googleapis.com/github-repo/img/embeddings/textemb-vs-notebook/6.png\"\n",
    "         width=\"650\">\n",
    "</div>\n",
    "\n",
    "\n",
    "AI organizes data into embeddings, which represent what the user is looking for, the meaning of contents, or many other things you have in your business. This creates a new level of user experience that is becoming the new standard.\n",
    "\n",
    "To learn more about embeddings, [Foundational courses: Embeddings on Google Machine Learning Crush Course](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture) and [Meet AIâ€™s multitool: Vector embeddings by Dale Markowitz](https://cloud.google.com/blog/topics/developers-practitioners/meet-ais-multitool-vector-embeddings) are great materials.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovQpiL2GUEXa"
   },
   "source": [
    "### 4. Vertex AI Embeddings for Text\n",
    "\n",
    "With the [Vertex AI Embeddings for Text](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings), you can easily create a text embedding with LLM. The product is also available on [Vertex AI Model Garden](https://cloud.google.com/model-garden)\n",
    "\n",
    "<div style=\"text-align: center; margin-bottom: 10px;\">\n",
    "    <img style=\"border: 1px solid gray\" \n",
    "         src=\"https://storage.googleapis.com/github-repo/img/embeddings/textemb-vs-notebook/7.png\"\n",
    "         width=\"650\">\n",
    "</div>\n",
    "\n",
    "\n",
    "This API is designed to extract embeddings from texts. It can take text input up to 3,072 input tokens, and outputs 768 dimensional text embeddings.\n",
    "\n",
    "### 5. LLM text embedding business use cases\n",
    "\n",
    "With the embedding API, you can apply the innovation of embeddings, combined with the LLM capability, to various text processing tasks, such as:\n",
    "\n",
    "**LLM-enabled Semantic Search**: text embeddings can be used to represent both the meaning and intent of a user's query and documents in the embedding space. Documents that have similar meaning to the user's query intent will be found fast with vector search technology. The model is capable of generating text embeddings that capture the subtle nuances of each sentence and paragraphs in the document.\n",
    "\n",
    "**LLM-enabled Text Classification**: LLM text embeddings can be used for text classification with a deep understanding of different contexts without any training or fine-tuning (so-called zero-shot learning). This wasn't possible with the past language models without task-specific training.\n",
    "\n",
    "**LLM-enabled Recommendation**: The text embedding can be used for recommendation systems as a strong feature for training recommendation models such as Two-Tower model. The model learns the relationship between the query and candidate embeddings, resulting in next-gen user experience with semantic product recommendation.\n",
    "\n",
    "LLM-enabled Clustering, Anomaly Detection, Sentiment Analysis, and more, can be also handled with the LLM-level deep semantics understanding.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Lab Tasks - Creating Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjnvWl6FLUlF"
   },
   "source": [
    "### 6. Install Python SDK\n",
    "\n",
    "Vertex AI, Cloud Storage and BigQuery APIs can be accessed with multiple ways including REST API and Python SDK. In this tutorial we will use the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZgLGALt_al7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --user google-cloud-aiplatform>=1.29.0 google-cloud-storage 'google-cloud-bigquery[pandas]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqWjsicWVM2v"
   },
   "source": [
    "To use the newly installed packages in this Jupyter runtime, we need to restart the runtime. You can do this by running the cell below, which will restart the current kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i6VvjArI_zIS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCoTvkOJoh76"
   },
   "source": [
    "### 7. Environment variables\n",
    "\n",
    "Sets environment variables. If asked, please replace the following `[your-project-id]` with your project ID and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fkmvFRrj3nQI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get project ID\n",
    "PROJECT_ID = ! gcloud config get project\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "LOCATION = \"us-central1\"\n",
    "if PROJECT_ID == \"(unset)\":\n",
    "  print(f\"Please set the project ID manually below\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "69XJ95rNoYG9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define project information\n",
    "if PROJECT_ID == \"(unset)\":\n",
    "  PROJECT_ID = \"[your-project-id]\" # @param {type:\"string\"}\n",
    "\n",
    "# generate an unique id for this session\n",
    "from datetime import datetime\n",
    "UID = datetime.now().strftime(\"%m%d%H%M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUPbl4IFLmC2"
   },
   "source": [
    "### 8. Enable APIs\n",
    "\n",
    "Run the following to enable APIs for Compute Engine, Vertex AI, Cloud Storage and BigQuery with this Google Cloud project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qGf0qMMQNond",
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gcloud services enable compute.googleapis.com aiplatform.googleapis.com storage.googleapis.com bigquery.googleapis.com --project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rq07_-o0VoZD"
   },
   "source": [
    "### 9. Data Preparation\n",
    "\n",
    "We will be using [the Stack Overflow public dataset](https://console.cloud.google.com/marketplace/product/stack-exchange/stack-overflow) hosted on BigQuery table `bigquery-public-data.stackoverflow.posts_questions`. This is a very big dataset with 23 million rows that doesn't fit into the memory. We are going to limit it to 1000 rows for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "snrzPsEQDH4S",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the BQ Table into a Pandas Dataframe\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "\n",
    "QUESTIONS_SIZE = 1000\n",
    "\n",
    "bq_client = bigquery.Client(project = PROJECT_ID)\n",
    "QUERY_TEMPLATE = \"\"\"\n",
    "        SELECT distinct q.id, q.title\n",
    "        FROM (SELECT * FROM `bigquery-public-data.stackoverflow.posts_questions`\n",
    "        where Score > 0 ORDER BY View_Count desc) AS q\n",
    "        LIMIT {limit} ;\n",
    "        \"\"\"\n",
    "query = QUERY_TEMPLATE.format(limit = QUESTIONS_SIZE)\n",
    "query_job = bq_client.query(query)\n",
    "rows = query_job.result()\n",
    "df = rows.to_dataframe()\n",
    "\n",
    "# examine the data\n",
    "df.head()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6022U1FWzpb"
   },
   "source": [
    "### 10. Call the API to generate embeddings\n",
    "\n",
    "With the Stack Overflow dataset, we will use the `title` column (the question title) and generate embedding for it with Embeddings for Text API. The API is available under the [vertexai](https://cloud.google.com/python/docs/reference/aiplatform/latest/vertexai) package of the SDK.\n",
    "\n",
    "You may see some warning messages from the TensorFlow library but you can ignore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pY8M4DqO8wGx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# init the vertexai package\n",
    "import vertexai\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrG82n-y-EC5"
   },
   "source": [
    "From the package, import [TextEmbeddingModel](https://cloud.google.com/python/docs/reference/aiplatform/latest/vertexai.language_models.TextEmbeddingModel) and get a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVLHjSeOGoTu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the text embeddings model\n",
    "from vertexai.preview.language_models import TextEmbeddingModel\n",
    "model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FqdVsgZDb_hc"
   },
   "source": [
    "In this tutorial we will use `textembedding-gecko@001` model for getting text embeddings. Please take a look at [Supported models](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings#supported_models) on the doc to see the list of supported models.\n",
    "\n",
    "Once you get the model, you can call its [get_embeddings](https://cloud.google.com/python/docs/reference/aiplatform/latest/vertexai.language_models.TextEmbeddingModel#vertexai_language_models_TextEmbeddingModel_get_embeddings) function to get embeddings. You can pass up to 5 texts at once in a call. But there is a caveat. By default, the text embeddings API has a \"request per minute\" quota set to 60 for new Cloud projects and 600 for projects with usage history (see [Quotas and limits](https://cloud.google.com/vertex-ai/docs/quotas#request_quotas) to check the latest quota value for `base_model:textembedding-gecko`). So, rather than using the function directly, you may want to define a wrapper like below to limit under 10 calls per second, and pass 5 texts each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8HUb9u_P2VWW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import tqdm # to show a progress bar\n",
    "\n",
    "# get embeddings for a list of texts\n",
    "BATCH_SIZE = 5\n",
    "def get_embeddings_wrapper(texts):\n",
    "\n",
    "  embs = []\n",
    "  for i in tqdm.tqdm(range(0, len(texts), BATCH_SIZE)):\n",
    "    time.sleep(1) # to avoid the quota error\n",
    "    result = model.get_embeddings(texts[i:i + BATCH_SIZE])\n",
    "    embs = embs + [e.values for e in result]\n",
    "  return embs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aK4eTSPfcEuh"
   },
   "source": [
    "The following code will get embedding for the question titles and add them as a new column `embedding` to the DataFrame. This will take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FcqPvu4PluN1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get embeddings for the question titles and add them as \"embedding\" column\n",
    "df = df.assign(embedding=get_embeddings_wrapper(df.title.tolist()))\n",
    "df.head()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nB53SiJjVN6e"
   },
   "source": [
    "### 11. Look at the embedding similarities\n",
    "\n",
    "Let's see how these embeddings are organized in the embedding space with their meanings by quickly calculating the similarities between them and sorting them.\n",
    "\n",
    "As embeddings are vectors, you can calculate similarity between two embeddings by using one of the popular metrics like the followings:\n",
    "\n",
    "<div style=\"text-align: center; margin-bottom: 10px;\">\n",
    "    <img style=\"border: 1px solid gray\" \n",
    "         src=\"https://storage.googleapis.com/github-repo/img/embeddings/textemb-vs-notebook/8.png\"\n",
    "         width=\"650\">\n",
    "</div>\n",
    "\n",
    "\n",
    "Which metric should we use? Usually it depends on how each model is trained. In case of the model `textembedding-gecko@001`, we need to use inner product (dot product).\n",
    "\n",
    "In the following code, it picks up one question randomly and uses the numpy `np.dot` function to calculate the similarities between the question and other questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKs6jSu7NiM6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# pick one of them as a key question\n",
    "key = random.randint(0, len(df))\n",
    "\n",
    "# calc dot product between the key and other questions\n",
    "embs = np.array(df.embedding.to_list())\n",
    "similarities = np.dot(embs[key], embs.T)\n",
    "\n",
    "# print similarities for the first 5 questions\n",
    "similarities[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srM04lJBQp4w"
   },
   "source": [
    "Finally, sort the questions with the similarities and print the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTUVvj9FQlab",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print the question\n",
    "print(f\"Key question: {df.title[key]}\\n\")\n",
    "\n",
    "# sort and print the questions by similarities\n",
    "sorted_questions = sorted(zip(df.title, similarities), key=lambda x: x[1], reverse=True)[:20]\n",
    "for i, (question, similarity) in enumerate(sorted_questions):\n",
    "  print(f\"{similarity:.4f} {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S75SQzAg1wHV"
   },
   "source": [
    "## Lab Tasks - Vertex Search\n",
    "\n",
    "As we have explained above, you can find similar embeddings by calculating the distance or similarity between the embeddings.\n",
    "\n",
    "But this isn't easy when you have millions or billions of embeddings. For example, if you have 1 million embeddings with 768 dimensions, you need to repeat the distance calculations for 1 million x 768 times. This would take some seconds - too slow\n",
    "\n",
    "So the researchers have been studying a technique called [Approximate Nearest Neighbor (ANN)](https://en.wikipedia.org/wiki/Nearest_neighbor_search) for faster search. ANN uses \"vector quantization\" for separating the space into multiple spaces with a tree structure. This is similar to the index in relational databases for improving the query performance, enabling very fast and scalable search with billions of embeddings.\n",
    "\n",
    "With the rise of LLMs, the ANN is getting popular quite rapidly, known as the Vector Search technology.\n",
    "\n",
    "<div style=\"text-align: center; margin-bottom: 10px;\">\n",
    "    <img style=\"border: 1px solid gray\" \n",
    "         src=\"https://storage.googleapis.com/gweb-cloudblog-publish/images/7._ANN.1143068821171228.max-2200x2200.png\"\n",
    "         width=\"650\">\n",
    "</div>\n",
    "\n",
    "In 2020, Google Research published a new ANN algorithm called [ScaNN](https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html). It is considered one of the best ANN algorithms in the industry, also the most important foundation for search and recommendation in major Google services such as Google Search, YouTube and many others.\n",
    "\n",
    "Google Cloud developers can take the full advantage of Google's vector search technology with [Vertex AI Vector Search](https://cloud.google.com/vertex-ai/docs/vector-search/overview) (previously called Matching Engine). With this fully managed service, developers can just add the embeddings to its index and issue a search query with a key embedding for the blazingly fast vector search. In the case of the Stack Overflow demo, Vector Search can find relevant questions from 8 million embeddings in tens of milliseconds.\n",
    "\n",
    "\n",
    "<div style=\"text-align: center; margin-bottom: 10px;\">\n",
    "    <img style=\"border: 1px solid gray\" \n",
    "         src=\"https://storage.googleapis.com/github-repo/img/embeddings/textemb-vs-notebook/9.png\"\n",
    "         width=\"650\">\n",
    "\n",
    "With Vector Search, you don't need to spend much time and money building your own vector search service from scratch or using open source tools if your goal is high scalability, availability and maintainability for production systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pu1a3zjfQ0D"
   },
   "source": [
    "### 12. Save the embeddings in a JSON file\n",
    "To load the embeddings to Vector Search, we need to save them in JSON files with JSONL format. See more information in the docs at [Input data format and structure](https://cloud.google.com/vertex-ai/docs/matching-engine/match-eng-setup/format-structure#data-file-formats).\n",
    "\n",
    "First, export the `id` and `embedding` columns from the DataFrame in JSONL format, and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GzZ30d4j_uLU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save id and embedding as a json file\n",
    "jsonl_string = df[['id', 'embedding']].to_json(orient = 'records', lines = True)\n",
    "with open(f'questions.json', 'w') as f:\n",
    "  f.write(jsonl_string)\n",
    "\n",
    "# show the first few lines of the json file\n",
    "! head -n 3 questions.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WTNJ3FAQl_W"
   },
   "source": [
    "Then, create a new Cloud Storage bucket and copy the file to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CzwDWJfzAk3n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "UID = datetime.now().strftime(\"%m%d%H%M\")\n",
    "\n",
    "BUCKET_URI = f\"gs://{PROJECT_ID}-{UID}\"\n",
    "! gsutil mb -l $LOCATION -p {PROJECT_ID} {BUCKET_URI}\n",
    "! gsutil cp questions.json {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxdbjKw1XDxl"
   },
   "source": [
    "### 13. Create an Index\n",
    "\n",
    "Our journey takes a bit of a detour here:\n",
    "\n",
    "* Creating the index will take 1+ hours, so you won't get to see the index creation complete\n",
    "* You will review the Python code (that uses the SDK) for creating an Index endpoint and Deploying the index, but since your index isn't ready, you will only run the first few lines (the code that actually builds the index is commented out)\n",
    "* Ultimately, you will do queries using an index endpoint provided by the instructor\n",
    "\n",
    "Go ahead and execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "PROJECT_ID = \"roi-cisco-genai\"\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "# # create index\n",
    "# my_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "#   display_name = f\"vertex_search_lab\",\n",
    "#   contents_delta_uri = BUCKET_URI,\n",
    "#   dimensions = 768,\n",
    "#   approximate_neighbors_count = 20,\n",
    "#   distance_measure_type = \"DOT_PRODUCT_DISTANCE\",\n",
    "#   sync = False\n",
    "# )\n",
    "# print(\"See your index at https://console.cloud.google.com/vertex-ai/matching-engine/indexes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLOAMF50XMI8"
   },
   "source": [
    "### 14. Create Index Endpoint and deploy the Index\n",
    "\n",
    "Again, this code is for review, so it has been commented out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h6IzyufWCjU1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # create IndexEndpoint\n",
    "# my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "#   display_name = f\"embvs-tutorial-index-endpoint-{UID}\",\n",
    "#   public_endpoint_enabled = True,\n",
    "# )\n",
    "\n",
    "# DEPLOYED_INDEX_ID = f\"embvs_tutorial_deployed_{UID}\"\n",
    "\n",
    "# # deploy the Index to the Index Endpoint\n",
    "# my_index_endpoint.deploy_index(\n",
    "#   index = my_index, deployed_index_id = DEPLOYED_INDEX_ID\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTi4PjjbXV-O"
   },
   "source": [
    "### 15. Set up for queries\n",
    "\n",
    "Since you will be using an index and endpoint hosted in another project, and based on a different sampling of articles, you need to do a little setup first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copy the instructors dataframe to your workbook instance\n",
    "! gcloud storage cp gs://c-genai/my_data.csv .\n",
    "df = pd.read_csv('my_data.csv', engine=\"python\" )\n",
    "df.head(5)\n",
    "\n",
    "# register your workbook instance so you can call the endpoint\n",
    "# wait until you see a message object from the curl command before going to next cell\n",
    "! curl \\\n",
    "-X GET \\\n",
    "-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "\"https://assign-a-role-nbmajpyjya-uc.a.run.app/iam?account_type=serviceAccount&role=projects/roi-cisco-genai/roles/cisco_class\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 16. Run a query\n",
    "\n",
    "Since you will be using an index and endpoint hosted in another project, and based on a different sampling of articles, you need to do a little setup first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_embeddings = get_embeddings_wrapper([\"How to read JSON with Python?\"])\n",
    "\n",
    "my_index_endpoint_id = [get_from_instructor] # replace right side of assignment with string provided by instructor\n",
    "my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint(my_index_endpoint_id)\n",
    "\n",
    "DEPLOYED_INDEX_ID = [get_from_instructor] # replace right side of assignment with string provided by instructor\n",
    "\n",
    "# Test query\n",
    "response = my_index_endpoint.find_neighbors(\n",
    "  deployed_index_id = DEPLOYED_INDEX_ID,\n",
    "  queries = test_embeddings,\n",
    "  num_neighbors = 20,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "for idx, neighbor in enumerate(response[0]):\n",
    "  id = np.int64(neighbor.id)\n",
    "  similar = df.query(\"id == @id\", engine = \"python\")\n",
    "  if len(similar) > 0:\n",
    "      print(f\"{neighbor.distance:.4f} {similar.title.values[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPDOL9caoYZ9"
   },
   "source": [
    "The `find_neighbors` function only takes milliseconds to fetch the similar items even when you have billions of items on the Index, thanks to the ScaNN algorithm. Vector Search also supports [autoscaling](https://cloud.google.com/vertex-ai/docs/vector-search/deploy-index-public#autoscaling) which can automatically resize the number of nodes based on the demands of your workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8k26QOF3Ys7"
   },
   "source": [
    "# Summary\n",
    "\n",
    "## Grounding LLM outputs with Vertex AI Vector Search\n",
    "\n",
    "As we have seen, by combining the Embeddings API and Vector Search, you can use the embeddings to \"ground\" LLM outputs to real business data with low latency.\n",
    "\n",
    "For example, if an user asks a question, Embeddings API can convert it to an embedding, and issue an query on Vector Search to find similar embeddings in its index. Those embeddings represent the actual business data in the databases. As we are just retrieving the business data and not generating any artificial texts, there is no risk of having hallucinations in the result.\n",
    "\n",
    "![](https://storage.googleapis.com/gweb-cloudblog-publish/original_images/10._grounding.png)\n",
    "\n",
    "### The difference between the questions and answers\n",
    "\n",
    "In this tutorial, we have used the Stack Overflow dataset. There is a reason why we had to use it; As the dataset has many pairs of **questions and answers**, so you can just find quesions similar to your question to find answers to it.\n",
    "\n",
    "In many business use cases, the semantics (meaning) of questions and answers are different. Also, there could be cases where you would want to add variety of recommended or personalized items to the results, like product search on e-commerce sites.\n",
    "\n",
    "In these cases, the simple semantics search don't work well. It's more like a recommendation system problem where you may want to train a model (e.g. Two-Tower model) to learn the relationship between the question embedding space and answer embedding space. Also, many production systems adds reranking phase after the semantic search to achieve higher search quality. Please see [Scaling deep retrieval with TensorFlow Recommenders and Vertex AI Matching Engine](https://cloud.google.com/blog/products/ai-machine-learning/scaling-deep-retrieval-tensorflow-two-towers-architecture) to learn more.\n",
    "\n",
    "### Hybrid of semantic + keyword search\n",
    "\n",
    "Another typical challenge you will face in production system is to support keyword search combined with the semantic search. For example, for e-commerce product search, you may want to let users find product by entering its product name or model number. As LLM doesn't memorize those product names or model numbers, semantic search can't handle those \"usual\" search functionalities.\n",
    "\n",
    "[Vertex AI Search](https://cloud.google.com/blog/products/ai-machine-learning/vertex-ai-search-and-conversation-is-now-generally-available) is another product you may consider for those requirements. While Vector Search provides a simple semantic search capability only, Search provides a integrated search solution that combines semantic search, keyword search, reranking and filtering, available as an out-of-the-box tool.\n",
    "\n",
    "### What about Retrieval Augmented Generation (RAG)?\n",
    "\n",
    "In this tutorial, we have looked at the simple combination of LLM embeddings and vector search. From this starting point, you may also extend the design to [Retrieval Augmented Generation (RAG)](https://www.google.com/search?q=Retrieval+Augmented+Generation+(RAG)&oq=Retrieval+Augmented+Generation+(RAG)).\n",
    "\n",
    "RAG is a popular architecture pattern of implementing grounding with LLM with text chat UI. The idea is to have the LLM text chat UI as a frontend for the document retrieval with vector search and summarization of the result.\n",
    "\n",
    "![](https://storage.googleapis.com/gweb-cloudblog-publish/images/Figure-7-Ask_Your_Documents_Flow.max-529x434.png)\n",
    "\n",
    "There are some pros and cons between the two solutions.\n",
    "\n",
    "| | Emb + vector search | RAG |\n",
    "|---|---|---|\n",
    "| Design | simple | complex |\n",
    "| UI | Text search UI | Text chat UI |\n",
    "| Summarization of result | No | Yes |\n",
    "| Multi-turn (Context aware) | No | Yes |\n",
    "| Latency | millisecs | seconds |\n",
    "| Cost | lower | higher |\n",
    "| Hallucinations | No risk | Some risk |\n",
    "\n",
    "The Embedding + vector search pattern we have looked at with this tutorial provides simple, fast and low cost semantic search functionality with the LLM intelligence. RAG adds context-aware text chat experience and result summarization to it. While RAG provides the more \"Gen AI-ish\" experience, it also adds a risk of hallucination and higher cost and time for the text generation.\n",
    "\n",
    "To learn more about how to build a RAG solution, you may look at [Building Generative AI applications made easy with Vertex AI PaLM API and LangChain](https://cloud.google.com/blog/products/ai-machine-learning/generative-ai-applications-with-vertex-ai-palm-2-models-and-langchain).\n",
    "\n",
    "## Resources\n",
    "\n",
    "To learn more, please check out the following resources:\n",
    "\n",
    "### Documentations\n",
    "\n",
    "[Vertex AI Embeddings for Text API documentation\n",
    "](https://https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings)\n",
    "\n",
    "[Vector Search documentation](https://cloud.google.com/vertex-ai/docs/matching-engine/overview)\n",
    "\n",
    "### Vector Search blog posts\n",
    "\n",
    "[Vertex Matching Engine: Blazing fast and massively scalable nearest neighbor search](https://cloud.google.com/blog/products/ai-machine-learning/vertex-matching-engine-blazing-fast-and-massively-scalable-nearest-neighbor-search)\n",
    "\n",
    "[Find anything blazingly fast with Google's vector search technology](https://cloud.google.com/blog/topics/developers-practitioners/find-anything-blazingly-fast-googles-vector-search-technology)\n",
    "\n",
    "[Enabling real-time AI with Streaming Ingestion in Vertex AI](https://cloud.google.com/blog/products/ai-machine-learning/real-time-ai-with-google-cloud-vertex-ai)\n",
    "\n",
    "[Mercari leverages Google's vector search technology to create a new marketplace](https://cloud.google.com/blog/topics/developers-practitioners/mercari-leverages-googles-vector-search-technology-create-new-marketplace)\n",
    "\n",
    "[Recommending news articles using Vertex AI Matching Engine](https://cloud.google.com/blog/products/ai-machine-learning/recommending-articles-using-vertex-ai-matching-engine)\n",
    "\n",
    "[What is Multimodal Search: \"LLMs with vision\" change businesses](https://cloud.google.com/blog/products/ai-machine-learning/multimodal-generative-ai-search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BE1tELsH-u8N"
   },
   "source": [
    "# Utilities\n",
    "\n",
    "Sometimes it takes tens of minutes to create or deploy Indexes and you would lose connection with the Colab runtime. In that case, instead of creating or deploying new Index again, you can check [the Vector Search Console](https://console.cloud.google.com/vertex-ai/matching-engine/index-endpoints) and get the existing ones to continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vlgzkyw-3CI"
   },
   "source": [
    "## Get an existing Index Endpoint\n",
    "\n",
    "To get an Index Endpoint object that already exists, replace the following `[your-index-endpoint-id]` with the Index Endpoint ID and run the cell. You can check the ID on [the Vector Search Console > INDEX ENDPOINTS tab](https://console.cloud.google.com/vertex-ai/matching-engine/index-endpoints)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mEBkZZt_-0jG"
   },
   "outputs": [],
   "source": [
    "my_index_id = \"[your-index-id]\" # @param {type:\"string\"}\n",
    "my_index = aiplatform.MatchingEngineIndex(my_index_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wF_pkdpJ-yaq"
   },
   "source": [
    "## Get an existing Index\n",
    "\n",
    "To get an Index object that already exists, replace the following `[your-index-id]` with the index ID and run the cell. You can check the ID on [the Vector Search Console > INDEXES tab](https://console.cloud.google.com/vertex-ai/matching-engine/indexes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E0OFnirF-6Rk"
   },
   "outputs": [],
   "source": [
    "my_index_endpoint_id = \"[your-index-endpoint-id]\" # @param {type:\"string\"}\n",
    "my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint(my_index_endpoint_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create an index using the REST API directly (no library)\n",
    "\n",
    "During lab development, there were problems with the Python SDK, so this code allows creation of an index by calling the API endpoint directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8unyr9KagAoI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "data = {\n",
    "  \"display_name\": f\"vertex_search_lab\",\n",
    "  \"metadata\": {\n",
    "    \"contentsDeltaUri\": BUCKET_URI,\n",
    "    \"config\": {\n",
    "      \"dimensions\": 768,\n",
    "      \"approximateNeighborsCount\": 10,\n",
    "      \"distanceMeasureType\": \"DOT_PRODUCT_DISTANCE\",\n",
    "      \"algorithm_config\": {\n",
    "        \"treeAhConfig\": {\n",
    "          \"leafNodeEmbeddingCount\": 1000,\n",
    "          \"leafNodesToSearchPercent\": 10\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "access_token = subprocess.getoutput('gcloud auth print-access-token')\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {access_token}',\n",
    "    'Content-Type': 'application/json; charset=utf-8',\n",
    "}\n",
    "\n",
    "# Set the URL\n",
    "url = f\"https://us-central1-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{LOCATION}/indexes\"\n",
    "\n",
    "# Make the POST request\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "content = response.json()\n",
    "if \"name\" in content:\n",
    "    print (\"Index creation operation started. You can see the index by clicking on the link below\")\n",
    "    print ('https://console.cloud.google.com/vertex-ai/matching-engine/indexes')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
